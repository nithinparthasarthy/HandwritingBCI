{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook applies a large-vocabulary bigram language model (50k words) implemented in Kaldi to the RNN outputs.\n",
    "#It also measures character/word error rates after the language model has been applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = 'handwritingBCIData/'\n",
    "#rootDir = os.path.expanduser('.') + '/handwritingBCIData/'\n",
    "\n",
    "\n",
    "#point this towards the code directory\n",
    "repoDir = os.getcwd() + '/'\n",
    "\n",
    "#defines which train/test partition to use\n",
    "cvPart = 'HeldOutTrials'\n",
    "\n",
    "#defines which datasets to process\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5.2019.05.08\n",
      "t5.2019.11.25\n",
      "t5.2019.12.09\n",
      "t5.2019.12.11\n",
      "t5.2019.12.18\n",
      "t5.2019.12.20\n",
      "t5.2020.01.06\n",
      "t5.2020.01.08\n"
     ]
    }
   ],
   "source": [
    "#First we have to convert the RNN outputs to a format that Kaldi can read. Here we make a Kaldi-readable matrix\n",
    "#for each sentence that contains the RNN outputs. \n",
    "from characterDefinitions import getHandwritingCharacterDefinitions\n",
    "from characterDefinitionsOrig import getHandwritingCharacterDefinitionsOrig\n",
    "from rnnEval import rnnOutputToKaldiMatrices\n",
    "\n",
    "#charDef = getHandwritingCharacterDefinitions()\n",
    "    \n",
    "allErrCounts = []\n",
    "outDir = rootDir+'RNNTrainingSteps/Step5_RNNInference/' + cvPart\n",
    "\n",
    "for x in range(len(dataDirs)):\n",
    "    print(dataDirs[x])\n",
    "    \n",
    "    if \"IamOnline\" in dataDirs[x]:\n",
    "       charDef = getHandwritingCharacterDefinitions()\n",
    "    else:\n",
    "       charDef = getHandwritingCharacterDefinitionsOrig() \n",
    "    outputs = scipy.io.loadmat(outDir + '/' + dataDirs[x] + '_inferenceOutputs.mat')\n",
    "    sentenceDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDirs[x]+'/sentences.mat')\n",
    "    \n",
    "    #define where to save the Kaldi matrices\n",
    "    kaldiDir = rootDir+'RNNTrainingSteps/Step6_ApplyBigramLM/' + cvPart + '/KaldiMatrices/' + dataDirs[x] + '/'\n",
    "    if not os.path.isdir(kaldiDir):\n",
    "        os.mkdir(kaldiDir)\n",
    "\n",
    "    #This function converts the RNN output to kaldi matrices so the language model can process it.\n",
    "    #Note that as part of this process we use the RNN's character start signal output to create a fake 'CTC blank' signal\n",
    "    #for the language model (the language model was originally designed to be used with a CTC loss).\n",
    "    cProb = rnnOutputToKaldiMatrices(outputs['outputs'], \n",
    "                             sentenceDat['numTimeBinsPerSentence']/2+50, \n",
    "                             charDef, \n",
    "                             kaldiDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize what a Kaldi matrix looks like. Note that the first row is the CTC-blank signal, which goes low only briefly whenever\n",
    "#there is a new character.\n",
    "#plt.figure(figsize=(12,4))\n",
    "#plt.imshow(cProb[122].T, aspect='auto', clim=[0,-15])\n",
    "#plt.xlabel('Time Step')\n",
    "#plt.ylabel('Character Log Probability\\n(31 characters + CTC blank)')\n",
    "#plt.colorbar(fraction=0.046, pad=0.04)\n",
    "#plt.title('Example Kaldi Probability Matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t5.2019.05.08\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RNNTrainingSteps/bashScratch/lmDecode_0.sh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bcb6cc4ed3da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m#this utility function generates temporary .sh files for running parallel decoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mparallelBash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscriptFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbashFilePrefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnParallelProcesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m#make sure we have permissions to execute everything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\handwritingBCI-main\\parallelBash.py\u001b[0m in \u001b[0;36mparallelBash\u001b[1;34m(argList, scriptFile, bashFilePrefix, nProcesses)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprocessIdx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnProcesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbashFilePrefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessIdx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.sh'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnPerProcess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RNNTrainingSteps/bashScratch/lmDecode_0.sh'"
     ]
    }
   ],
   "source": [
    "#Now call kaldi to apply the language model to the probability matrices created above.\n",
    "#Note that you will need kaldi (https://github.com/kaldi-asr/kaldi) \n",
    "#AND you will need custom kaldi decoders from https://github.com/jpuigcerver/kaldi-decoders.\n",
    "from parallelBash import parallelBash\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "#Add kaldi and kaldi-decoders binaries to the path. This assumes that kaldi is located in the home directory.\n",
    "homeDir = os.path.expanduser('~/')\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi-decoders/bin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/bin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/lm'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/lmbin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/fstbin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/featbin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/gmmbin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/tools/openfst-1.6.7/bin'\n",
    "os.environ['PATH'] += ':'+homeDir+'kaldi/src/latbin'\n",
    "\n",
    "#make sure we have permission to execute all the scripts we'll be calling below\n",
    "#os.system('chmod +x ' + repoDir+'kaldiLMScripts/bigramLmDecode.sh')\n",
    "#os.system('chmod +x ' + repoDir+'kaldiLMScripts/parseOptions.inc.sh')\n",
    "#os.system('chmod +x ' + repoDir+'kaldiLMScripts/remove_transcript_dummy_boundaries.sh')\n",
    "#os.system('chmod +x ' + repoDir+'kaldiLMScripts/int2sym.pl')\n",
    "\n",
    "#for each dataset, we launch multiple instances of the kaldi decoder program in parallel, each operating on a single sentence\n",
    "for dataDir in dataDirs:\n",
    "    print(dataDir)\n",
    "\n",
    "    #where the language model files are stored\n",
    "    langModelDir = rootDir+'BigramLM'\n",
    "    \n",
    "    #where the kaldi probability matrices are saved (these are inputs for this step)\n",
    "    matsDir = rootDir+'RNNTrainingSteps/Step6_ApplyBigramLM/'+cvPart+'/KaldiMatrices/'+dataDir\n",
    "    \n",
    "    #where we should save the language model outputs (which are lists of candidate sentences, along with their scores)\n",
    "    outDir = rootDir+'RNNTrainingSteps/Step6_ApplyBigramLM/'+cvPart+'/KaldiOutput/'+dataDir\n",
    "    if not os.path.isdir(outDir):\n",
    "        os.mkdir(outDir)\n",
    "        \n",
    "    #clear previous LM outputs\n",
    "    # os.system('del ' + outDir + '/*') # Nithin: Replaced rm by del\n",
    "\n",
    "    #This next bit of code generates some temporary .sh files that help launch multiple decoding programs at once.\n",
    "    #we then launch the '_master.sh' file to kick everything off, and wait until the outputs are all there before continuing\n",
    "    #to the next dataset.\n",
    "    scriptFile = repoDir + 'kaldiLMScripts/bigramLmDecode.sh'\n",
    "    \n",
    "    if not os.path.isdir(rootDir + 'RNNTrainingSteps/bashScratch'):\n",
    "        os.mkdir(rootDir + 'RNNTrainingSteps/bashScratch')\n",
    "    bashFilePrefix = rootDir + 'RNNTrainingSteps/bashScratch/lmDecode'\n",
    "    bashFilePrefix = 'RNNTrainingSteps/bashScratch/lmDecode'\n",
    "    \n",
    "    fileList = os.listdir(matsDir)\n",
    "    txtFiles = []\n",
    "    for f in fileList:\n",
    "        if f.endswith('.txt'):\n",
    "            txtFiles.append(f)\n",
    "            \n",
    "    nFiles = len(txtFiles)    \n",
    "    nParallelProcesses = int(multiprocessing.cpu_count()/2)\n",
    "\n",
    "    sentenceIdx = np.arange(0,nFiles).astype(np.int32)\n",
    "    \n",
    "    argList = []\n",
    "    for x in range(len(sentenceIdx)):\n",
    "        newArgs = {}\n",
    "        newArgs['acoustic_scale'] = 1.0 #1.0, 1.79\n",
    "        newArgs['beam'] = 65\n",
    "        newArgs['max_active'] = 5000 #500000 #5000000\n",
    "        newArgs['1_mainArg'] = langModelDir\n",
    "        newArgs['2_mainArg'] = matsDir + '/kaldiMat_'+str(sentenceIdx[x])+'.txt'\n",
    "        newArgs['3_mainArg'] = outDir + '/' + str(sentenceIdx[x]) + '_'\n",
    "\n",
    "        argList.append(newArgs)\n",
    "\n",
    "    #this utility function generates temporary .sh files for running parallel decoding\n",
    "    parallelBash(argList, scriptFile, bashFilePrefix, nParallelProcesses)\n",
    "        \n",
    "    #make sure we have permissions to execute everything\n",
    " #   os.system('chmod +x ' + bashFilePrefix+'_master.sh')\n",
    "#    for x in range(nParallelProcesses):\n",
    "  #      os.system('chmod +x ' + bashFilePrefix+'_'+str(x)+'.sh')\n",
    "            \n",
    "    #launching the master file starts everything\n",
    "    os.system(bashFilePrefix+'_master.sh')\n",
    "\n",
    "    #now we just sit and wait until we have all the outputs\n",
    "    numFilesInDir = 0\n",
    "    while numFilesInDir < nFiles*9:\n",
    "        numFilesInDir = int(len(os.listdir(outDir)))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have all the language model outputs, load them up and compute character/word error rates\n",
    "from kaldiReadWrite import readKaldiLatticeFile, readKaldiAliFile\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from rnnEval import wer\n",
    "import warnings\n",
    "\n",
    "#this stops scipy.io.savemat from throwing a warning about empty entries\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#keep track of the error counts for all validation sentences in 'valErrCounts' so we can summarize in the next cell\n",
    "valErrCounts = []\n",
    "\n",
    "for dataDir in dataDirs:\n",
    "    #process ALL sentences from this dataset (both train & test)\n",
    "    print(' --' + dataDir + '-- ')\n",
    "\n",
    "    sentenceDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/sentences.mat')\n",
    "    # cvPartFile = scipy.io.loadmat(rootDir+'RNNTrainingSteps/trainTestPartitions_'+cvPart+'.mat')\n",
    "    cvPartFile = scipy.io.loadmat('local/'+dataDir+'_trainTest.mat')\n",
    "    valIdx = cvPartFile[dataDir+'_test']\n",
    "    \n",
    "    allErrCounts = []\n",
    "    decSentences = []\n",
    "    \n",
    "    kaldiDir = rootDir+'RNNTrainingSteps/Step6_ApplyBigramLM/'+cvPart+'/KaldiOutput/'+dataDir \n",
    "    nFiles = int(len(os.listdir(kaldiDir))/9)   \n",
    "    \n",
    "    for fileIdx in range(nFiles):\n",
    "        nbestFile = kaldiDir+'/'+str(fileIdx)+'_transcript.txt'\n",
    "        acFile = kaldiDir+'/'+str(fileIdx)+'_best_acscore.ark'\n",
    "        lmFile = kaldiDir+'/'+str(fileIdx)+'_best_lmscore.ark'\n",
    "\n",
    "        nums, content = readKaldiLatticeFile(nbestFile, 'string')\n",
    "        _, acScore = readKaldiLatticeFile(acFile, 'numeric')\n",
    "        _, lmScore = readKaldiLatticeFile(lmFile, 'numeric')\n",
    "\n",
    "        #here we select the best-scoring sentence according to the formula 'acScore + 2.0*lmScore'\n",
    "        bestIdx = np.argmin(acScore + 2.0*lmScore)\n",
    "        decSent = content[bestIdx]\n",
    "        decSentences.append(decSent)\n",
    "        \n",
    "        trueText = sentenceDat['sentencePrompt'][fileIdx,0][0]\n",
    "        trueText = trueText.replace('>',' ')\n",
    "        trueText = trueText.replace('~','.')\n",
    "        trueText = trueText.replace('#','')\n",
    "        \n",
    "        #compute character/word error counts\n",
    "        charErrs = wer(trueText, decSent)\n",
    "        wordErrs = wer(trueText.split(), decSent.split())\n",
    "        allErrCounts.append(np.array([charErrs, len(trueText), wordErrs, len(trueText.split())]))\n",
    "\n",
    "        #print the language model outputs for all sentences in the held-out set\n",
    "        if fileIdx in valIdx:\n",
    "            print('#' + str(fileIdx))\n",
    "            print('True:    ' + trueText)\n",
    "            print('Decoded: ' + decSent)\n",
    "            print('')\n",
    "            \n",
    "            valErrCounts.append(np.array([charErrs, len(trueText), wordErrs, len(trueText.split())]))\n",
    "\n",
    "    #save error rates & decoded sentences for this dataset\n",
    "    concatCounts = np.stack(allErrCounts, axis=0)\n",
    "\n",
    "    saveDict = {}\n",
    "    saveDict['decSentences'] = decSentences\n",
    "    saveDict['trueSentences'] = sentenceDat['sentencePrompt']\n",
    "    saveDict['charCounts'] = concatCounts[:,1]\n",
    "    saveDict['charErrors'] = concatCounts[:,0]\n",
    "    saveDict['wordCounts'] = concatCounts[:,3]\n",
    "    saveDict['wordErrors'] = concatCounts[:,2]\n",
    "\n",
    "    scipy.io.savemat(rootDir + 'RNNTrainingSteps/Step6_ApplyBigramLM/' + cvPart + '/' + dataDir + '_errCounts.mat', saveDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize character error rate and word error rate across all held-out trials from all datasets\n",
    "concatErrCounts = np.squeeze(np.stack(valErrCounts, axis=0))\n",
    "cer = 100*(np.sum(concatErrCounts[:,0]) / np.sum(concatErrCounts[:,1]))\n",
    "wer = 100*(np.sum(concatErrCounts[:,2]) / np.sum(concatErrCounts[:,3]))\n",
    "\n",
    "print('Character error rate: %1.2f%%' % float(cer))\n",
    "print('Word error rate: %1.2f%%' % float(wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
